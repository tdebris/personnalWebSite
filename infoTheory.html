<html>
<head>
  <meta charset="utf-8">
  <title>Introduction to information theory </title>
</head>
 <div id="menu">
 </div>
<center>
<h2> Introduction to information theory<br>
</h2>
</center>
<p> <br>
 </p>
 <h3>Introduction</h3>

This course provides an introduction to information theory whose foundation were laid by C. Shannon. Information theory deals with finding the fundamental limits for compressing a signal, storing data or communicating information reliably over a noisy channel. It turns out that all these limits can be expressed in terms of a single quantity: the entropy.<br>

We will cover during this course Shannon's major results. Since Shannon's results information theory has found applications in many areas. We will discuss some of them, such as statistics and cryptography.
<p> <br> </p>


<h3>Acknowledgment</h3>
I am particularly grateful to Nicolas Sendrier and Jean-Pierre Tillich for sharing with me their materials to build this course (as well as providing many advices).

<p><br>  </p> 
 

<h3>Lectures</h3>

<ul>
  <li> Introduction to information theory: <a href="infoTheory/S1.pdf"> Lecture 1</a> and 
  <a href="infoTheory/td1.pdf"> Exercise Session 1</a> <br>
    1) Discrete probabilities <br> 
    2) Overview of information theory <br> 
    3) Entropy, what else? <br> 
<br>

  
  <p> </p></li>
<li> Source coding theorem and first efficient compression algorithms: <a href="infoTheory/S2.pdf"> Lecture 2</a> and 
  <a href="infoTheory/td2.pdf"> Exercise Session 2</a> <br>
  1) Source coding theorem <br>
  2) Compression with symbol codes<br>
  3) Optimal source coding: Huffman coding <br>
  <br>

<p> </p> </li> 
<li> Typical sequences and Asymptotic Equipartition Property (AEP): <a href="infoTheory/S3.pdf"> Lecture 3</a> and 
  <a href="infoTheory/td3.pdf"> Exercise Session 3</a> <br>
  1) Some reminders<br> 
  2) Entropy and stochastic processes<br> 
  3) Asymptotic equipartition property<br> 
  4) Memoryless sources verify the AEP <br>
  5) Markov chains: general sources verifying the AEP <br>
<br>

<p> </p></li> 
<li> Compression: arithmetic coding <a href="infoTheory/S4.pdf"> Lecture 4</a> <br>
  1) Some reminders: Huffman coding and Shannon source coding theorem<br> 
  2) Coding with intervals: Shannon-Fano-Elias and Shannon<br> 
  3) Arithmetic coding<br> 
  <br> 

<p> </p>  </li>  
<li> Method of types and applications: <a href="infoTheory/S5.pdf"> Lecture 5</a> and 
  <a href="infoTheory/td5.pdf"> Exercise Session 5</a> <br>
  1) Method of types<br>
  2) Alternative law of large numbers<br> 
  3) Universal coding<br>
  4) Large deviation theory <br> 
  5) Chernoff's bound <br>
  6) Sanov's theorem <br>
  7) Some applications of Sanov's theorem <br>
<br> 

<p> </p> </li> 
<li> Communication over a noisy channel: <a href="infoTheory/S6.pdf"> Lecture 6</a> and <a href="infoTheory/td6.pdf"> Exercise Session 6</a> <br>
  1) Noisy channels and capacity<br> 
  2) Noisy-channel coding theorem<br> 
  3) Proof of achievability<br>
  4) What is impossible<br>  
  5) Conclusion<br> 
<br> 

<p> </p> </li> 
 <li> Introduction to linear codes <a href="infoTheory/S7.pdf"> Lecture 7</a> and 
  <a href="infoTheory/td7.pdf"> Exercise Session 7</a> <br>
  1) Basics on linear codes <br> 
  2) Dual representation of linear codes<br> 
  3) Hamming distance/weight<br> 
  4) Bounds on minimum distance<br> 
  5) Reed-Solomon codes and their decoding algorithm<br>
<br> 


  <p> </p> </li> 

 <li> Random linear codes and Shannon's theorem <a href="infoTheory/S8.pdf"> Lecture 8</a> and 
  <a href="infoTheory/td8.pdf"> Exercise Session 8</a> <br>
  1) Shannon's theorem for linear codes<br> 
  2) About random codes<br> 
  3) Proof of Shannon's theorem for linear codes<br> 
  4) Random codes: a powerful tool<br>
  5) A little bit of cryptography<br>
<br> 
<p> </p> </li>
</ul>


<h3>Final exam</h3>
Presentation of one of the below topics. 
<ul>
  <li> Kolmogorov Complexity in Elements of Information Theory, Chapter 14, <br>
    Thomas M. Cover and Joy A. Thomas.
  </li><br> 

  <li> About code-based cryptography, <a href="https://www.math.u-bordeaux.fr/~gzemor/alekhnovich.pdf">Alekhnovich’s cryptosystems</a>, <br>
    in the lecture notes by Gilles Zémor.
  </li><br> 


  <li> A <a href="http://arizona.openrepository.com/arizona/handle/10150/607470">tutorial</a> about an important family of error correcting codes: LDPC codes.
  </li><br>
  

   <li> Lempel-Ziv compression algorithm in Elements of Information Theory, Chapter 13, <br>
  Thomas M. Cover and Joy A. Thomas.
</li><br>

<li> Quantum information theory: Chapter 11 up to 12.3 in Quantum Computation and Quantum Information, <br> 
   Michael A. Nielsen and Isaac L. Chuang.
  </li></br>


<li> Programming project as described at the end of <a href="infoTheory/S3.pdf"> Lecture 3</a>.
  </li></br>


</ul>



<h3>Bibliography (books and lectures notes used for this course)</h3>
<ul> 
<li> 
  &ldquo;Elements of Information Theory&rdquo;, Thomas M. Cover and Joy A. Thomas.</a>
</li>
<p> </p>

<li> 
  &ldquo; Information Theory, Inference, and Learning Algorithms&rdquo;, David J. C. MacKay.</a>
</li>
<p> </p>
  <li> Lecture notes by Nicolas Sendrier: <a href = "https://www.rocq.inria.fr/secret/Nicolas.Sendrier/thinfo.pdf"> https://www.rocq.inria.fr/secret/Nicolas.Sendrier/thinfo.pdf</a>
</li>
<p> </p>
</ul>



</body>
</html>
